<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="CS181W Final Project">
    <meta name="author" content="Tommy Truong, Luke Wilson, Haque Ishfaq, Eric Holmdahl">

    <title>Ethics of AI on the Battlefield</title>
    
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css">

    <!-- Optional theme -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap-theme.min.css">

    <link rel="stylesheet" href="theme.css">

  </head>

  <body role="document">

    <!-- Fixed navbar -->
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <span class="navbar-brand" href="#">AI on the Battlefield</span>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li><a href="index.html">Home</a></li>
            <li class="active"><a href="ethics.html">Ethics</a></li>
            <li><a href="policy.html">Policy</a></li>
            <li><a href="recommendation.html">Recommendation</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container theme-showcase" role="main">
      <h1>Ethics of Autonomous Weapons</h1>
      <p class="lead">
        The recent advancement of autonomous weapons in the battlefield have sparked ethical debates over their use. The debate over autonomous weapons is complex, and even within a specific ethical framework justifications can be made for either side. We will consider two ethical views in particular that were described in class, utilitarianism and deontology. For both of these perspectives, we will explore the arguments for and against the use of autonomous weapons in war.
      </p>
      <h2>Utilitarianism</h2>
      <p>
        Utilitarianism is an ethical perspective that considers an action morally right if it maximizes utility or happiness for the greatest number of people. Utilitarianism thus tends to favor the needs and wants of the many over those of the few. It is categorized as a consequentialist view because it views the morality of actions based only off of the consequences of the actions; utilitarianism does not take into account the motives or intent of the person carrying out the action. Under this view, an action that ultimately results in the greatest amount of happiness for the greatest number of people will always be morally correct. <a href="http://plato.stanford.edu/entries/consequentialism/">(Stanford Encyclopedia of Philosophy)</a>
      </p>
      <div role="tabpanel">
        <ul class="nav nav-tabs" role="tablist">
          <li role="presentation" class="active"><a href="#utilitarian-pro" aria-controls="utilitarian-pro" role="tab" data-toggle="tab">Arguments For</a></li>
          <li role="presentation"><a href="#utilitarian-con" aria-controls="utilitarian-con" role="tab" data-toggle="tab">Arguments Against</a></li>
        </ul>
        <div class="tab-content argument-content">
          <div role="tabpanel" class="tab-pane fade in active" id="utilitarian-pro">
            <p>
              One of the main utilitarian arguments in favor of both autonomous and semi-autonomous weapons is that they result in fewer deaths when compared to more traditional forms of warfare. Thus, because they bring benefits and reduce suffering for a larger number of people, their use is morally justifiable.              
            </p>
            <p>
              There is evidence supporting the claim that autonomous weapons result in fewer civilian deaths. Avery Plaw, a political scientist at the University of Massachusetts, found that civilian deaths due to drone strikes in Pakistan account for roughly 20% of total casualties, while conventional military conflicts in recent years have had civilian deaths totaling 30-80% of total casualties. In light of these statistics, Bradley J. Strawser, an assistant professor of philosophy at the Naval Postgraduate School, concludes that weapons using AI might &quot;not only [be] ethically permissible but also might be ethically obligatory, because of their advantages in identifying targets and striking with precision&quot; <a href="http://www.nytimes.com/2012/07/15/sunday-review/the-moral-case-for-drones.html">(Shane)</a>. This claim is also supported by the findings of a recent task force on US drone policy <a href="http://www.stimson.org/images/uploads/task_force_report_final_web_062414.pdf">(Abizaid)</a>. In their final report, they stated: &quot;We do not doubt that some US UAV strikes have killed innocent civilians. Nonetheless, the empirical evidence suggests that the number of civilians killed is small compared to the civilian deaths typically associated with other weapons delivery systems (including manned aircraft).&quot;
            </p>
            <p>
              Proponents of more autonomous weapons also argue that they will result in fewer casualties for military personnel. In a fully autonomous weapon system, machines and robots will carry out all of the work without any human intervention at all. And in supervised or semi-autonomous weapons, operators of the weapons can work remotely, away from the frontline, in &quot;perfect safety&quot; <a href="http://www.nytimes.com/2012/07/15/sunday-review/the-moral-case-for-drones.html">(Shane)</a>.
            </p>
            <p>
              Utilitarians can thus argue that by reducing the number of civilian and military deaths, the use of autonomous weapons on the battlefield is justified.
            </p>

          </div>
          <div role="tabpanel" class="tab-pane fade" id="utilitarian-con">
            <p>
              However, utilitarian arguments can also be made against the use of AI on the battlefield. Critics argue that the ease of deployment of autonomous weapons, combined with their perceived precision and accuracy, could result in their overuse and could thus ultimately result in more fatalities overall. These critics fear that drone strikes have become a &quot;convenient substitute for capture&quot; and could lead to &quot;unnecessary killings&quot; <a href="http://www.nytimes.com/2012/07/15/sunday-review/the-moral-case-for-drones.html">(Shane)</a>. This concern is also raised by the task force, who stated that &quot;[they] doubt that the United States would have engaged in nearly as many targeted strikes against suspected terrorists&quot; if drones were not an option, because &quot;airstrikes using manned aircraft would generally be viewed as creating an unacceptably high risk of civilian casualties&quot; <a href="http://www.stimson.org/images/uploads/task_force_report_final_web_062414.pdf">(Abizaid)</a>. If more civilian deaths occur due to overuse of drones, then from a utilitarian perspective the use of autonomous weapons would be morally wrong because of this consequence, even if the weapons were deployed with the intent of reducing civilian casualties.
            </p>
            <p>
              Opponents to semi-autonomous weapons could also argue that drone operators do not work in &quot;perfect safety&quot; because they can still suffer deleterious effects from operating the drones. The task force stated that &quot;[drone] operators are particularly vulnerable to post-traumatic stress [because] they may watch their targets for weeks or even months, seeing them go about the routines of daily life, before one day watching on-screen as they are obliterated&quot; <a href="http://www.stimson.org/images/uploads/task_force_report_final_web_062414.pdf">(Abizaid)</a>. Indeed, several operators have described how stressful and unsettling their jobs can be: one operator, Bruce Black, described the job as &quot;hours and days of boredom punctuated by a few moments of stark terror&quot;, and another operator, Michael Haas, wondered about how much guilt he should feel about his work and had to block such thoughts out of his mind for weeks <a href="http://www.theguardian.com/world/2015/feb/24/drone-warfare-life-on-the-new-frontline">(Woods)</a>. In light of these comments, opponents to drones can argue that drone operators can suffer comparable psychological trauma to those fighting on the front line, so the argument that drones have maximized these operators' utility is inaccurate.
            </p>
          </div>
        </div>
      </div>

      <h2>Deontology</h2>
      <p>
        Deontology is another ethical perspective that considers actions themselves (and not the consequences of the actions) in evaluating their moral correctness. Deontologists view that there are certain &quot;norms&quot; or rules that our actions should follow, and it is the duty and obligation of people to obey these rules. These norms determine actions that a person can do and actions that a person can never do. For instance, Immanuel Kant, who developed a lot of current deontological framework, claims that it is not alright to lie to others, because if everyone were supposed to lie to others, then no one would believe anyone else and it would be impossible to deceive others. <a href="http://plato.stanford.edu/entries/ethics-deontological/">(Stanford Encyclopedia of Phiosophy)</a>
      </p>
      <p>
        Another important tenet of Kant's deontological ethics is that people must never treat humanity (either in themselves or in others) as merely a means to accomplishing an end. People must respect other humans as ends themselves. This is in contrast to utilitarianism, which is criticized for allowing actions that benefit a large number of people at the expense of a few (for example, harvesting organs from an unwilling donor to save the lives of many recipients). <a href="http://plato.stanford.edu/entries/kant-moral/">(Stanford Encyclopedia of Philosophy)</a>
      </p>
      <div role="tabpanel">
        <ul class="nav nav-tabs" role="tablist">
          <li role="presentation" class="active"><a href="#deontology-pro" aria-controls="deontology-pro" role="tab" data-toggle="tab">Arguments For</a></li>
          <li role="presentation"><a href="#deontology-con" aria-controls="deontology-con" role="tab" data-toggle="tab">Arguments Against</a></li>
        </ul>
        <div class="tab-content argument-content">
          <div role="tabpanel" class="tab-pane fade in active" id="deontology-pro">
            <p>
              A deontological justification for the use of autonomous weapons is that the autonomous weapons can help ensure that ethical norms and rules are followed. Ron Arkin, a roboticist at Georgia Tech, argues that because all of us have a duty to respect other humans, it is the duty of scientists to &quot;look for effective ways to reduce s inhumanity to man through technology&quot; <a href="https://www.hrw.org/news/2014/08/21/killer-robots-keeping-control-autonomous-weapons">(Wareham)</a>. Arkin feels that further development of autonomous weapons can help fulfill this duty to humanity because autonomous robots can be programmed to follow the ethical laws of war more strictly than humans can <a href="https://www.hrw.org/news/2014/08/21/killer-robots-keeping-control-autonomous-weapons">(Wareham)</a>. Drones are already more precise and accurate than conventional bombing, so their use could be seen as showing more respect to the right to life of innocent civilians <a href="http://www.academia.edu/4287681/Moral_Case_for_Automated_Drones">(Andreotti)</a>. Using fully autonomous weapons would also remove soldiers from the frontline, where they may be likely to commit actions against their ethical duties because of the stress and trauma of battle. man
            </p>
          </div>
          <div role="tabpanel" class="tab-pane fade" id="deontology-con">
            <p>
However, opponents to autonomous weapons argue that these machines cannot follow moral or ethical rules. Jim Fetzer, a McKnight Professor Emeritus at the University of Minnesota, argues that autonomous weapons are &quot;inherently amoral &mdash; neither moral nor immoral &mdash; from a deontological point of view&quot; and that they &quot;have no concept of morality, of personhood or of mutual respect&quot; because they are just simply machines acting out their software and that the systems are not complex enough to make ethical decisions <a href="http://www.globalresearch.ca/on-the-ethical-conduct-of-warfare-predator-drones/23324">(Fetzer)</a>. Dr. Joanna Bryson of the University of Bath argues that it is infeasible to create machine that could follow rules of ethics: &quot;People think about Asimov's laws, but they were set up to point out how a simple ethical system doesn't work. If you read the short stories, every single one is about a failure, and they are totally impractical&quot; <a href="http://www.techrepublic.com/article/robots-of-death-robots-of-love-the-reality-of-android-soldiers-and-why-laws-for-robots-are-doomed-to-failure/">(Ranger)</a>. And if the systems are developed far enough so that they can make ethical decisions like humans, then they might be subject to the same imperfections that prevent humans from carrying out their ethical duty, thus losing the claimed ethical advantage over traditional warfare <a href="http://www.academia.edu/4287681/Moral_Case_for_Automated_Drones">(Andreotti)</a>.
            </p>
            <p>
              Another argument against autonomous weapons is that their use would not show respect to other humans because they could threaten the right of due process. Fetzer argues that &quot;the central consequence of a deontological perspective is <i>the centrality of due process</i>&quot; and that &quot;no one should be deprived of their life, liberty or property without an appropriate form of certification that punishment of that kind is something that they deserve&quot; <a href="http://www.globalresearch.ca/on-the-ethical-conduct-of-warfare-predator-drones/23324">(Fetzer)</a>. The use of autonomous weapons can threaten this right because current laws are not well-adapted to dealing with autonomous weapons. The task force on US drone policy writes about the dilemma suffered by individuals living in areas with targeted drone strikes:
            </p>
              <blockquote class="quote">&quot;Death can come from the sky at any moment, and the instability and incoherence of existing legal categories means that there is no way for an individual to be certain whether he is considered targetable by the United States... What's more, individuals in states such as Pakistan or Yemen have no ability to seek clarification of the law or their status from an effective or impartial legal system, no ability to argue that they have been mistakenly or inappropriately targeted or that the intelligence that led to their inclusion on a 'kill list' was flawed or fabricated, and no ability to seek redress for injury. Their national laws and courts can offer no assistance in the face of foreign power, and far from protecting their fundamental rights and freedoms, their own states may in fact be deceiving them about their knowledge of and cooperation with US strikes. Meanwhile, geography and finances make it impossible to access US courts, and a variety of legal barriers &mdash; such as the state secrets privilege, the political question doctrine, and issues of standing, ripeness and mootness &mdash; in any case would prevent meaningful access to justice.&quot; <a href="http://www.stimson.org/images/uploads/task_force_report_final_web_062414.pdf">(Abizaid)</a></blockquote>
          </div>
        </div>
      </div>


    </div> <!-- /container -->

    <!-- Latest compiled and minified JavaScript -->
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>


  </body>
</html>
