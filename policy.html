<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="CS181W Final Project">
    <meta name="author" content="Tommy Truong, Luke Wilson, Haque Ishfaq, Eric Holmdahl">

    <title>AI on the Battlefield - Policy</title>
    
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css">

    <!-- Optional theme -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap-theme.min.css">

    <link rel="stylesheet" href="theme.css">

  </head>

  <body role="document">

    <!-- Fixed navbar -->
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">AI on the Battlefield</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li><a href="index.html">Home</a></li>
            <li><a href="ethics.html">Ethics</a></li>
            <li class="active"><a href="policy.html">Policy</a></li>
            <li><a href="recommendation.html">Recommendation</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container theme-showcase" role="main">

      <h2>Autonomous Weapon Policies</h2>
      <p class="intro-text">
        The global community is in the midst of discussing the future of AI on the battlefield. Most specifically, the world is deliberating over the use of Lethal Autonomous Weapon Systems (LAWS); that is, weapon systems that act out-of-the-loop and are able to target and destroy humans and objectives on their own. Since there are no true LAWS in use today, much time is being spent defining what a LAWS is and whether or not existing International Humanitarian Law (IHL) already prohibits inhumane LAWS use.
      </p>
      <p class="intro-text">
        Over the past 2 years the Convention on Certain Conventional Weapons (CCW), also known as the Inhumane Weapons Convention, has focused discussion on LAWS. Many countries are currently in dialogue about what should be done and whether or not a preemptive ban or moratorium should be put in place. While there is CCW precedence for preemptively banning weapons with the <a href="https://www.icrc.org/applic/ihl/ihl.nsf/Treaty.xsp?action=openDocument&documentId=70D9427BB965B7CEC12563FB0061CFB2" target="_blank">Protocol IV against Blinding Laser Weapons</a>, most countries find themselves on the fence because not much is known about what LAWS may be able to do in the future. NGO groups like the <a href="http://www.stopkillerrobots.org" target="_blank">Campaign to Stop Killer Robots</a> are present in these discussions in an attempt to convince states to push LAWS regulation through the CCW and UN.      
      </p>
      <h3>Pro-LAWS</h3>
      <p>
        Those who find themselves on the Pro-LAWS side of the debate share many of the same sentiments. Their arguments usually revolve around the fact that lethal autonomous weapon systems, as currently defined, is too broad and unknown. Pro-LAWS states would prefer to further develop the technology to see what can be done. Many also stand behind International Humanitarian Law, believing that its regulations on force proportionality, discrimination, military necessity, and humanity are enough to guard against any inhumane use of LAWS.
      </p>
      <p>
        Most states find themselves on the fence and would prefer to learn more about the potential of LAWS before regulating or banning them altogether. Some of these states agree that humans should always be in-the-loop at some point, and that it is inhumane for a machine to choose whether or not a human should live. Others have explicitly said that they will never develop LAWS without a human in-the-loop, but they also believe that the technology is too new and emergent to be able to call it inhumane and ban it. 
      </p>
      <div role="tabpanel">
        <ul class="nav nav-tabs" role="tablist">
          <li role="presentation" class="active"><a href="#us" aria-controls="us" role="tab" data-toggle="tab">US</a></li>
          <li role="presentation"><a href="#israel" aria-controls="israel" role="tab" data-toggle="tab">Israel</a></li>
          <li role="presentation"><a href="#uk" aria-controls="uk" role="tab" data-toggle="tab">UK</a></li>
        </ul>
        <div class="tab-content argument-content">
          <div role="tabpanel" class="tab-pane fade in active" id="us">

            <p>
              In 2012 the US released the <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.dtic.mil%2Fwhs%2Fdirectives%2Fcorres%2Fpdf%2F300009p.pdf&sa=D&sntz=1&usg=AFQjCNGPGTOD-jpq-2pw-__WkPwJ_L5iWQ" target="_blank">Department of Defense Directive 3000.09</a> which establishes a policy for how the United States will assign responsibilities for the development of lethal autonomous weapon systems. The directive also outlines additional weapon system requirements designed to minimize risks of LAWS failure. While the directive neither encourages nor prohibits the development of such systems, it puts in place a framework for future LAWS development.
            </p>
            <p>
              In the most recent Convention on Certain Conventional Weapons meeting, the US states that the CCW is a great venue for further discussion and education about the emerging technologies in LAWS, but that it would be premature to make any regulations. 
            </p>
            <p>
              The United States believes that a robust internal policy process and methodology can help mitigate risk when developing new weapon systems. Throughout the CCW they hope to elaborate and share their review process so that the world can ensure certain standards, like International Humanitarian Laws, are met in future LAWS development.
            </p>
            <p>
              Read the full statement <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.unog.ch%2F80256EDD006B8954%2F(httpAssets)%2F8B33A1CDBE80EC60C1257E2800275E56%2F%24file%2F2015_LAWS_MX_USA%2Bbis.pdf&sa=D&sntz=1&usg=AFQjCNFYIEIY-vTPtM8vAgleESBKLL538w" target="_blank">here</a>.
            </p>

          </div>
          <div role="tabpanel" class="tab-pane fade" id="israel">
            <p>
              Israel declared its Pro-LAWS policy by cautioning the Convention on Certain Conventional Weapons from banning weapon systems that do not yet exist. They see a promising future in the use of LAWS, and believe that the current International Humanitarian Law will bar any unethical LAWS from being used in warfare.
            </p>
            <p>
              Israel prefaces its opinion by saying that discussing technologies that do not yet exist is a challenging task and deliberations about future Lethal Autonomous Weapon Systems need to be guided by a number of assumptions.
            </p>
            <p>
              First, Israel believes that the world should enter the discussion with an open mind, considering both the potential risks and positive capabilities of LAWS. It’s nearly impossible to see what autonomous weapon systems will look like fifty years in the future, so all discussions should be done prudently.
            </p>
            <p>
              Second, Israel says that the use of LAWS in the future must comply with the rules of International Humanitarian Law, just like every other weapon. It is important to keep this in mind before creating new regulations around their use, as this may be detrimental to further development of LAWS that could have a positive impact on warfare, and may even further comply with International Humanitarian Law. States that develop LAWS should subject their weapon system to an internal legal review.  
            </p>
            <p>
              One of Israel’s final points is that future LAWS will take on a large variety of capabilities and behaviors. The assessment of these weapon systems should be done on a case by case basis, and thus any discussion about the future of LAWS should take this into account.
            </p>
            <p>
              Read the full statement <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.unog.ch%2F80256EDD006B8954%2F(httpAssets)%2F1B879A0827FBA307C1257E29004778B8%2F%24file%2F2015_LAWS_MX_Israel_GS%2Bbis.pdf&sa=D&sntz=1&usg=AFQjCNGIzhBe8orvPGSzx1eOmrqkfApVOA" target="_blank">here</a>.
            </p>
          </div>
          <div role="tabpanel" class="tab-pane fade" id="uk">

            <p>
              At the most recent CCW in April 2015, the UK stood firmly in their belief that there is no need for additional LAWS regulation, despite the fact that they always plan on having a human in-the-loop. The UK believes that humans should always be the ones making a final decision but can be supported by a system that "has the appropriate level of automation.” They call this an intelligent partnership.
            </p>
            <p>
              The UK concludes that it takes it obligations under IHL very seriously and that all of its new weapon systems undergo rigorous testing to ensure compliance with IHL and <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.article36.org%2Fabout%2F&sa=D&sntz=1&usg=AFQjCNFQ89xXzYyFwCdMy51eE3nvZPdZLw" target="_blank">Article 36</a> protocols.
            </p>
            <p>
              The Foreign Office told the <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.theguardian.com%2Fpolitics%2F2015%2Fapr%2F13%2Fuk-opposes-international-ban-on-developing-killer-robots&sa=D&sntz=1&usg=AFQjCNH6ypRUroQrmqWLMB8YTS28ig-_Bw" target="_blank">Guardian</a>: "At present, we do not see the need for a prohibition on the use of Laws, as international humanitarian law already provides sufficient regulation for this area.
            </p>
            <p>
              "The United Kingdom is not developing lethal autonomous weapons systems, and the operation of weapons systems by the UK armed forces will always be under human oversight and control. As an indication of our commitment to this, we are focusing development efforts on remotely piloted systems rather than highly automated systems."
            </p>
            <p>
              The UK ended their statement at the CCW with a nod to the future, saying that "Civilian industry is leading technological innovation in automation, and the military may adopt it for its own purposes as it evolves… To legislate now, without a clear understanding of the potential opportunities as well as dangers of a technology that we cannot fully appreciate, would risk leading to the use of generalised and unclear language which would be counter-productive.”
            </p>
            <p>
              Read the full statement <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.unog.ch%2F80256EDD006B8954%2F(httpAssets)%2F3AA5E280106A73EFC1257E2900472797%2F%24file%2F2015_LAWS_MX_UK_IHL.pdf&sa=D&sntz=1&usg=AFQjCNGYSWoQQ-PQ-LlsTFFNurj7IsLDmw" target="_blank">here</a>.
            </p>

          </div>
        </div>
      </div>

      <h3>Anti-LAWS</h3>
      <p>
        Most fundamentally, those who are in favor of preemptively banning lethal autonomous weapon systems believe that they are immoral. Arguments often claim that machines lack human judgement, morals, and the ability to understand context and thus should not be allowed to choose whether or not human lives or dies. Additionally, many Anit-LAWS policies mention that autonomous weapons will lower the barrier-to-entry for high-tech militaries, causing more asymmetrical wars. Those with scientific backgrounds mention the current state of AI and how far it is from an AI that will be able to accurately target and kill enemies like a human soldier.
      </p>
      <div role="tabpanel">
        <ul class="nav nav-tabs" role="tablist">
          <li role="presentation" class="active"><a href="#stop-killer-robots" aria-controls="stop-killer-robots" role="tab" data-toggle="tab">Campaign to Stop Killer Robots</a></li>
          <li role="presentation"><a href="#pakistan" aria-controls="pakistan" role="tab" data-toggle="tab">Pakistan</a></li>
        </ul>
        <div class="tab-content argument-content">
          <div role="tabpanel" class="tab-pane fade in active" id="stop-killer-robots">
            <p>
              The Campaign to Stop Killer Robots is a group of 9 NGOs including the <a href="http://www.hrw.org/" target="_blank">Human Rights Watch</a>, <a href="http://www.article36.org/about/" target="_blank">Article 36</a>, and <a href="http://www.icrac.net" target="_blank">International Committee for Robot Arms Control</a> that have teamed up in an effort to preemptively ban the development and use of fully autonomous weapon systems. They believe that "giving machines the power to decide who lives and dies on the battlefield is and unacceptable application of technology” that "crosses a fundamental moral line.” The group also fears that if one of the world’s high-tech militaries like China, Israel, Russia, the UK, or the US deploys a fully autonomous weapon system, others may abandon policies of restraint which would lead to a robotic arms race.
            </p>
            <p>
              The Campaign to Stop Killer Robots believes that autonomous robots lack human judgement and the ability to understand context, which are necessary to make complex ethical choices and abide by International Humanitarian Law. Additionally, being able to replace human troops with machines may make the decision to go to war easier, further shifting the burden of war onto civilians.
            </p>
            <p>
              The campaign seeks to prohibit taking a human out-of-the-loop with respect to targeting and attack decisions with a preemptive ban on LAWS achieved through an international treaty.
            </p>
            <p>
              Key members of The Campaign to Stop Killer Robots include Noel E. Sharkey, Professor of Artificial Intelligence and Robotics at the University of Sheffield, UK. He has written many papers that discuss the use of autonomous weapon systems on the battlefield including <a href="https://www.icrc.org/eng/resources/documents/article/review-2012/irrc-886-sharkey.htm" target="_blank">The evitability of autonomous robot warfare</a>. Being an AI robotics scientist, most of his arguments center around the overestimation of AI’s current capabilities, explaining that computers are just now able to distinguish a human from a car, and that determining a civilian from a combatant is hardly conceivable at this time.
            </p>
            <p>
              Learn more about the Campaign to Stop Killer Robots <a href="http://www.stopkillerrobots.org/" target="_blank">here</a>.
            </p>
          </div>
          <div role="tabpanel" class="tab-pane fade" id="pakistan">
            <p>
              Of the many countries who seemed to not support out-of-the-loop LAWS at the CCW, Pakistan was the least ambiguous in its statement about autonomy in weapon systems. It states, "LAWS are by nature unethical… The question is simple: Should a machine programmed on a complex set of algorithms, which is devoid of the notions of morality and humanity, be allowed to decide who should live and who should die? We are convinced that the answer is a firm NO.”
            </p>
            <p>
              Pakistan goes on to argue that states with LAWS would resort to the use of force on a frequent basis as armed conflict would no longer be a measure of last resort, but a recurrent low-cost affair. Additionally, LAWS would be a detriment to disarmament and nonproliferation on WMDs. Countries with the prospect of being overwhelmed by LAWS would be reluctant to give up the WMD capabilities they have, while others would feel the need to acquire them. These alone would undermine the international peace and security that international law seeks to maintain. 
            </p>
            <p>
              Furthermore, LAWS create an accountability and transparency vacuum as those who use them would be unable to assign responsibility for the harm they cause. If something goes wrong, who would be held responsible: the programmer, the hardware manufacturer, the commander, or the user state? "If the nature of a weapon renders responsibility for its consequences impossible, it’s use should be considered unethical and unlawful.”
            </p>
            <p>
              Read the full statement <a href="http://www.unog.ch/80256EDD006B8954/(httpAssets)/C6F268A1B1D7B80BC1257E26005E33E4/$file/Statement+on+LAWS+-+CCW+Informal+Meeting+of+Experts+April+2015.pdf" target="_blank">here</a>.
            </p>
          </div>
        </div>
      </div>


    </div> <!-- /container -->

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
  </body>
</html>
